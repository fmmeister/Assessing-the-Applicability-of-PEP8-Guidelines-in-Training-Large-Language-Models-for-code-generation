{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlxO-E4WQ39t",
    "outputId": "42784d55-3764-48b2-ed98-dbeaf29e6266"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: trl==0.11.4 in /usr/local/lib/python3.11/dist-packages (0.11.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.11.4) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.11.4) (4.50.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from trl==0.11.4) (1.5.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.11.4) (3.5.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.11.4) (0.9.18)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from trl==0.11.4) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (4.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.11.4) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.11.4) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.0->trl==0.11.4) (4.67.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.4) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.4) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.4) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.4) (4.4.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->trl==0.11.4) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.11.4) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.11.4) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.11.4) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.11.4) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.11.4) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.11.4) (3.11.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->trl==0.11.4) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.11.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.11.4) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.11.4) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.11.4) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "!pip install trl==0.11.4\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "\n",
    "class GeneratorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class (inherits from torch.utils.data) for the generator.\n",
    "    Consists of 'prompts', 'code' and 'test_list' of the mbpp dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: dict, tokenizer, padding_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding_length = padding_length\n",
    "        self.prompts = torch.Tensor(\n",
    "            tokenizer(data['prompts'], padding='max_length', max_length=padding_length['prompts'])['input_ids'])\n",
    "        self.code = torch.Tensor(\n",
    "            tokenizer(data['code'], padding='max_length', max_length=padding_length['code'])['input_ids'])\n",
    "        self.test_list = torch.Tensor(\n",
    "            tokenizer(data['test_list'], padding='max_length', max_length=padding_length['test_list'])['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'prompts': self.prompts[idx],\n",
    "                'code': self.code[idx],\n",
    "                'test_list': self.test_list[idx]}\n",
    "\n",
    "    def select_for_discriminator(self, indices, tokenizer, padding_length, ground_truth):\n",
    "        if isinstance(self.code, torch.Tensor):\n",
    "            # handling bug, when prepare_data() was called two times,\n",
    "            # but each time there was a different structure of self.code\n",
    "            # no idea what the fuck is happening\n",
    "            self.code = list(torch.unbind(Tensor.int(self.code), dim=0))\n",
    "            self.code = tokenizer.batch_decode(self.code, skip_special_tokens=True)\n",
    "        elif isinstance(self.code, list):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Error with the structure of the code.\")\n",
    "        return DiscriminatorDataset({\"code\": [self.code[i] for i in indices],\n",
    "                                     \"ground_truth\": [ground_truth for _ in indices]},\n",
    "                                    tokenizer=tokenizer, padding_length=padding_length)\n",
    "\n",
    "\n",
    "class DiscriminatorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class (inherits from torch.utils.data) for the discriminator.\n",
    "    Consists of 'code' and a 'ground_truth' (1 for real data, 0 for generated data).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: dict, tokenizer, padding_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding_length = padding_length\n",
    "        self.code = torch.Tensor(tokenizer(data['code'], padding='max_length',\n",
    "                                           max_length=padding_length['code'])['input_ids'])\n",
    "        self.ground_truth = torch.Tensor(tokenizer(data['ground_truth'], padding='max_length',\n",
    "                                                   max_length=padding_length['code'])['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.code)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'code': self.code[idx],\n",
    "                'ground_truth': self.ground_truth[idx]}\n",
    "\n",
    "    def __add__(self, other):\n",
    "        self.code = list(torch.unbind(Tensor.int(self.code), dim=0))\n",
    "        self.code = self.tokenizer.batch_decode(self.code)\n",
    "\n",
    "        other.code = list(torch.unbind(Tensor.int(other.code), dim=0))\n",
    "        other.code = other.tokenizer.batch_decode(other.code)\n",
    "\n",
    "        self.ground_truth = list(torch.unbind(Tensor.int(self.ground_truth), dim=0))\n",
    "        self.ground_truth = self.tokenizer.batch_decode(self.ground_truth)\n",
    "\n",
    "        other.ground_truth = list(torch.unbind(Tensor.int(other.ground_truth), dim=0))\n",
    "        other.ground_truth = other.tokenizer.batch_decode(other.ground_truth)\n",
    "\n",
    "        return DiscriminatorDataset(\n",
    "            {\"code\": self.code + other.code, \"ground_truth\": self.ground_truth + other.ground_truth},\n",
    "            self.tokenizer, self.padding_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def interleave(dataset_generated, dataset_groundtruth, num_samples, tokenizer, padding_length):\n",
    "        if len(dataset_generated) == len(dataset_groundtruth):\n",
    "            sample_ids = random.sample(range(len(dataset_groundtruth) + len(dataset_generated)), num_samples)\n",
    "            interleave_dataset = dataset_generated + dataset_groundtruth\n",
    "\n",
    "            interleave_dataset.code = list(torch.unbind(Tensor.int(interleave_dataset.code), dim=0))\n",
    "            interleave_dataset.code = tokenizer.batch_decode(interleave_dataset.code)\n",
    "\n",
    "            interleave_dataset.ground_truth = list(torch.unbind(Tensor.int(interleave_dataset.ground_truth), dim=0))\n",
    "            interleave_dataset.ground_truth = tokenizer.batch_decode(interleave_dataset.ground_truth)\n",
    "\n",
    "            return DiscriminatorDataset({\"code\": [interleave_dataset.code[i] for i in sample_ids],\n",
    "                                         \"ground_truth\": [interleave_dataset.ground_truth[i] for i in sample_ids]},\n",
    "                                        tokenizer=tokenizer, padding_length=padding_length)\n",
    "        else:\n",
    "            return Exception\n",
    "\n",
    "\n",
    "def get_index_length_datasets(train: bool, code_parrot: bool, mbpp: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Needed for max_lenth parameter for tokenization; specific to mbpp dataset.\n",
    "    \"\"\"\n",
    "    if mbpp:\n",
    "        if code_parrot:\n",
    "            if train:\n",
    "                return {'prompts': 49, 'code': 252, 'test_list': 302}  # for codeparrot/codeparrot-small tokenizer\n",
    "            else:\n",
    "                return {'prompts': 51, 'code': 0, 'test_list': 2248}\n",
    "        else:\n",
    "            if train:\n",
    "                return {'prompts': 50, 'code': 289, 'test_list': 350}  # for Qwen/Qwen2.5-0.5B tokenizer\n",
    "            else:\n",
    "                return {'prompts': 47, 'code': 0, 'test_list': 3670}\n",
    "    else:\n",
    "        if code_parrot:\n",
    "            if not train:\n",
    "                return {'prompts': 391, 'code': 0, 'test_list': 235}  # for codeparrot/codeparrot-small tokenizer +\n",
    "                # humaneval dataset for evaluation\n",
    "\n",
    "\n",
    "def prepare_data(generator: AutoModelForCausalLMWithValueHead,\n",
    "                 ground_dataset: GeneratorDataset, num_samples: int,\n",
    "                 gen_args: dict, tokenizer: AutoTokenizer, train: bool,\n",
    "                 code_parrot: bool) -> DiscriminatorDataset:\n",
    "    \"\"\"\n",
    "    Called each epoch during training of the discriminator; builds new DiscriminatorDataset,\n",
    "    which consists of samples from the dataset and generated samples with the current generator.\n",
    "    \"\"\"\n",
    "    gen_args['num_return_sequences'] = num_samples\n",
    "\n",
    "    padding_length = get_index_length_datasets(train, code_parrot, mbpp=True)\n",
    "\n",
    "    generated_samples = sample_from_generation(generator, gen_args, tokenizer, padding_length)\n",
    "    ground_truth_samples = sample_from_dataset(ground_dataset, num_samples, tokenizer, padding_length)\n",
    "    dataset = DiscriminatorDataset.interleave(generated_samples, ground_truth_samples, num_samples * 2, tokenizer,\n",
    "                                              padding_length)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def sample_from_generation(generator: AutoModelForCausalLMWithValueHead, gen_args: dict, tokenizer: AutoTokenizer,\n",
    "                           padding_length: dict) -> DiscriminatorDataset:\n",
    "    \"\"\"\n",
    "    Generates num_samples * samples (ground_truth = 0) with the current generator, which has been saved in the last epoch.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        samples = generator.generate(**gen_args)\n",
    "\n",
    "    samples = tokenizer.batch_decode(samples, skip_special_tokens=True)\n",
    "    sample_dicts = {\"code\": samples, \"ground_truth\": [\"0\" for _ in samples]}\n",
    "\n",
    "    return DiscriminatorDataset(sample_dicts, tokenizer=tokenizer, padding_length=padding_length)\n",
    "\n",
    "\n",
    "def sample_from_dataset(dataset: GeneratorDataset, num_samples: int, tokenizer: AutoTokenizer,\n",
    "                        padding_length: dict) -> DiscriminatorDataset:\n",
    "    \"\"\"\n",
    "    Returns num_samples * samples (ground_truth = 1) from the dataset.\n",
    "    \"\"\"\n",
    "    sample_ids = random.sample(range(len(dataset)), num_samples)\n",
    "    samples = dataset.select_for_discriminator(sample_ids, tokenizer, padding_length, ground_truth=\"1\")\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def load_gen_data(file_path: str, tokenizer: AutoTokenizer, train: bool, code_parrot: bool,\n",
    "                  mbpp: bool) -> GeneratorDataset:\n",
    "    \"\"\"\n",
    "    Process dataset of the following structure to GeneratorDataset.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data_dicts = json.load(file)\n",
    "    data_dicts['code'] = [\"\" for _ in range(0, len(data_dicts['test_list']))]\n",
    "    data_dicts['test_list'] = [str(item) for item in data_dicts['test_list']]\n",
    "    padding_length = get_index_length_datasets(train, code_parrot=code_parrot, mbpp=mbpp)\n",
    "\n",
    "    dataset = GeneratorDataset(data_dicts, tokenizer=tokenizer, padding_length=padding_length)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pycodestyle in /usr/local/lib/python3.11/dist-packages (2.13.0)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import importlib.util\n",
    "import re\n",
    "from io import StringIO\n",
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer\n",
    "!pip install pycodestyle\n",
    "from pycodestyle import Checker\n",
    "\n",
    "\n",
    "def get_rewards(generation_tensor: List[torch.Tensor],\n",
    "                generation_text: List[str],\n",
    "                device,\n",
    "                discriminator: torch.nn.Module,\n",
    "                avg_rewards_during_batches: list,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                padding_length: dict,\n",
    "                disc_weight: int,\n",
    "                temp_file_path: List[str],\n",
    "                test_list: List[str]) -> tuple[torch.Tensor, List[float]]:\n",
    "    \"\"\"\n",
    "    Collect general reward for the generator (combination of discriminator reward and objective reward).\n",
    "    \"\"\"\n",
    "    samples = Tensor.int(torch.Tensor(\n",
    "        tokenizer(generation_text, padding='max_length', max_length=padding_length['code'])['input_ids']).to(device))\n",
    "    discriminator.to(device)\n",
    "    pred = discriminator.forward(samples)\n",
    "    # get predictions for positive class,\n",
    "    # the more the discriminator is certain, that the sample is real (1), the higher the reward for the generator\n",
    "    disc_reward = softmax(pred, dim=-1)[:, 1]\n",
    "\n",
    "    # mapping from [0, 1] to [-1, 1]\n",
    "    disc_reward_transformation = disc_reward * 2 - 1\n",
    "\n",
    "    if disc_weight == 1:\n",
    "        rewards = disc_reward_transformation\n",
    "        avg_rewards = disc_reward_transformation.tolist()\n",
    "    elif disc_weight == 0:\n",
    "        obj_rewards, avg_rewards = collect_rewards(generation_tensor,\n",
    "                                                   generation_text,\n",
    "                                                   discount=1,\n",
    "                                                   avg_rewards_during_batches=avg_rewards_during_batches,\n",
    "                                                   temp_file_path=temp_file_path,\n",
    "                                                   test_list=test_list)\n",
    "\n",
    "        rewards = obj_rewards.to(disc_reward_transformation.device)\n",
    "    else:\n",
    "        obj_rewards, avg_rewards = collect_rewards(generation_tensor,\n",
    "                                                   generation_text,\n",
    "                                                   discount=1,\n",
    "                                                   avg_rewards_during_batches=avg_rewards_during_batches,\n",
    "                                                   temp_file_path=temp_file_path,\n",
    "                                                   test_list=test_list)\n",
    "\n",
    "        obj_rewards = obj_rewards.to(disc_reward_transformation.device)\n",
    "\n",
    "        rewards = disc_weight * disc_reward_transformation + (1 - disc_weight) * obj_rewards\n",
    "\n",
    "    return rewards, avg_rewards\n",
    "\n",
    "\n",
    "def collect_rewards(generation_tensor: List[torch.Tensor],\n",
    "                    generation_text: List[str],\n",
    "                    avg_rewards_during_batches: List[float],\n",
    "                    temp_file_path: List[str],\n",
    "                    test_list: List[str],\n",
    "                    discount: float = 1.0) -> tuple[torch.Tensor, List[float]]:\n",
    "    \"\"\"\n",
    "    Collects rewards of the objective function (pep8 / try_test_list).\n",
    "    \"\"\"\n",
    "    collected = torch.zeros(len(generation_text))\n",
    "    reward_list_during_epoch = []\n",
    "    for k, sample in enumerate(temp_file_path):\n",
    "\n",
    "        try:\n",
    "            pep08_reward, output = pep08(sample)  # pep8 reward and output\n",
    "\n",
    "            with open(sample, \"a\") as temp_file:\n",
    "                # code = temp_file.read()\n",
    "                temp_file.write(\"\\n\\n# Errorcodes: \" + str(output) + \"\\n# pep08_reward: \" + str(pep08_reward))\n",
    "            # test_list_reward = try_test_list(sample, test_list[k])\n",
    "\n",
    "            #length_penalty = (generation_tensor[k].shape[0] / 100) - 1\n",
    "            # looks up token length of generated code and maps it to [-1, 1] (max_new_token_length = 200)\n",
    "\n",
    "            reward_list_during_epoch.append(pep08_reward)\n",
    "            rewards = torch.tensor(pep08_reward)\n",
    "            collected[k] = torch.sum(torch.tensor(rewards)) * torch.pow(torch.tensor(discount), torch.tensor(k))\n",
    "        except UnicodeDecodeError:\n",
    "            collected[k] = torch.tensor(-1)\n",
    "\n",
    "    if not reward_list_during_epoch:\n",
    "        print(\"No rewards collected\")\n",
    "    else:\n",
    "        avg_rewards_during_batches.append(sum(reward_list_during_epoch) / len(reward_list_during_epoch))\n",
    "    return collected, avg_rewards_during_batches\n",
    "\n",
    "\n",
    "def compilable(temp_file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Can the code be compiled by the standard python compiler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        compile(temp_file_path, \"<string>\", \"exec\")\n",
    "        reward = 100\n",
    "    except SyntaxError:\n",
    "        reward = -5\n",
    "    except ValueError:\n",
    "        reward = -5\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "class Capturing(list):\n",
    "    \"\"\"\n",
    "    Context Manager to capture pycodestyle's print statements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.pep08_output = self._stringio.getvalue()\n",
    "        del self._stringio\n",
    "        sys.stdout = self._stdout\n",
    "        # Filter specific error codes and store them in the list\n",
    "        error_code_pattern = r'\\b[EWF]\\d{3}\\b'  # Regex for error codes like E123, W456, F789\n",
    "        self.extend(re.findall(error_code_pattern, self.pep08_output))\n",
    "\n",
    "\n",
    "def pep08(temp_file_path: str) -> tuple[int, List[str]]:\n",
    "    \"\"\"\n",
    "    How much does the code adhere to pep08 standards.\n",
    "    \"\"\"\n",
    "    checker = Checker(temp_file_path, show_source=False, show_pep8=True)\n",
    "    num_errors = 15\n",
    "    with Capturing() as output:\n",
    "        try:\n",
    "            num_errors = checker.check_all()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while checking the code: {e}\")\n",
    "\n",
    "    if num_errors == 0:\n",
    "        reward = 1\n",
    "    elif num_errors <= 3 & num_errors > 0:\n",
    "        reward = 0.5\n",
    "    elif num_errors <= 5 & num_errors > 3:\n",
    "        reward = 0.25\n",
    "    elif num_errors <= 10 & num_errors > 5:\n",
    "        reward = -0.5\n",
    "    elif num_errors <= 15 & num_errors > 10:\n",
    "        reward = -0.75\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    return reward, output\n",
    "\n",
    "\n",
    "def try_test_list(temp_file_path: str, test_list: str) -> float:\n",
    "    \"\"\"\n",
    "    Function tries to find a function in the generated file and uses 'test_list' from mbpp dataset to evaluate the code.\n",
    "    \"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(temp_file_path, \"./temp_files/\" + temp_file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "    except Exception as e:\n",
    "        return -1.0  # Error loading the file\n",
    "\n",
    "    functions = [func for func in dir(module) if callable(getattr(module, func))]\n",
    "    if not functions:\n",
    "        return -0.5  # No functions found in the file.\n",
    "\n",
    "    for func_name in functions:\n",
    "        func = getattr(module, func_name)\n",
    "        if callable(func):\n",
    "            target_function = func\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        test_list = ast.literal_eval(test_list)\n",
    "        try:\n",
    "            for test in test_list:\n",
    "                exec(test)\n",
    "            return 1  # All tests passed!\n",
    "        except AssertionError:\n",
    "            return 0.2  # A test failed!\n",
    "    else:\n",
    "        return -0.25  # No matching function found.\n",
    "\n",
    "# ==================== Code Metrics =====================\n",
    "# (from https://radon.readthedocs.io/en/latest/intro.html)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tq37AvjQ39t",
    "outputId": "1df00986-945b-4991-e9b1-1a8b711881b8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from pycodestyle import Checker\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def log_error(message, verbose: bool) -> None:\n",
    "    \"\"\"\n",
    "    Log an error message to the console if verbose is True.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "\n",
    "def loading_bar(current, total, width=50) -> None:\n",
    "    \"\"\"\n",
    "    Custom progress bar for generating test files.\n",
    "    \"\"\"\n",
    "    progress = (current / total)\n",
    "    progress_bar = \"[\" + \"=\" * int(progress * width) + \" \" * (width - int(progress * width)) + \"]\"\n",
    "    percent = progress * 100\n",
    "    print(f\"\\r{progress_bar} {percent:.1f}%\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "def mbpp_sample_generieren(idx: int, device: str, dataset_test: GeneratorDataset,\n",
    "                           generator: PPOTrainer, tokenizer: AutoTokenizer,\n",
    "                           generation_kwargs: dict, destination_dir_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate code files from mbpp dataset.\n",
    "    \"\"\"\n",
    "    prompts_tensor = dataset_test.__getitem__(idx)['prompts'].to(torch.int64).to(device)\n",
    "    generation_tensor = generator.generate(query_tensor=prompts_tensor, return_prompt=False, **generation_kwargs)\n",
    "    generated_txts = tokenizer.batch_decode(generation_tensor, skip_special_tokens=True)\n",
    "    if not os.path.exists(destination_dir_path):\n",
    "        os.makedirs(destination_dir_path)\n",
    "    with open(f\"{destination_dir_path}/sample{idx}.py\", \"w\", encoding='utf-8') as file:\n",
    "        file.write(generated_txts[0])\n",
    "\n",
    "\n",
    "def eval_pep8(directory: str) -> tuple[float, str, float]:\n",
    "    \"\"\"\n",
    "    Check all files in directory for pep8 errors.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"Der Pfad {directory} ist kein gÃ¼ltiger Ordner.\")\n",
    "        return -1.0, \"\", 0.0\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    count = 0\n",
    "    zero_errors = 0\n",
    "    num_error_list = []\n",
    "    error_type_list = []\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        checker = Checker(file_path, show_source=False, show_pep8=True)\n",
    "        with Capturing() as output:\n",
    "            try:\n",
    "                num_errors = checker.check_all()\n",
    "                num_error_list.append(num_errors)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while checking the code: {e}\")\n",
    "        if num_errors == 0:\n",
    "            zero_errors += 1\n",
    "        error_type_list.append(output)\n",
    "        count += 1\n",
    "    all_errors = [error for file_errors in error_type_list for error in file_errors]\n",
    "    error_counts = Counter(all_errors)\n",
    "    most_common_error, frequency = error_counts.most_common(1)[0]\n",
    "\n",
    "    print(f\"The most common error is '{most_common_error}' with {frequency} occurrences.\")\n",
    "    print(f\"The compliance rate (0 errors) is {round((zero_errors / count) * 100, 2)}%.\")\n",
    "    print(\"Average pep8 errors found: \", round(sum(num_error_list) / count, 2))\n",
    "    return round(sum(num_error_list) / count, 2), most_common_error, round((zero_errors / count) * 100, 2)\n",
    "\n",
    "\n",
    "def extract_functions_from_code(code, verbose=False):\n",
    "    \"\"\"\n",
    "    Extract all functions defined in the code using regex.\n",
    "    \"\"\"\n",
    "    functions = []\n",
    "    try:\n",
    "        # Use regex to find all function definitions\n",
    "        matches = re.finditer(r\"def\\s+(\\w+)\\s*\\([^)]*\\)\\s*:\", code)\n",
    "        for match in matches:\n",
    "            functions.append(match.group(1))  # Extract the function name\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error extracting functions from code: {e}\", verbose)\n",
    "    return functions\n",
    "\n",
    "\n",
    "def extract_function_code(code, function_name, verbose=False):\n",
    "    \"\"\"\n",
    "    Extract the code for a specific function from the code file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex to find the function definition and its body\n",
    "        pattern = rf\"def\\s+{function_name}\\s*\\([^)]*\\)\\s*:\\s*((?:\\n\\s+.*)+)\"\n",
    "        match = re.search(pattern, code, re.DOTALL)\n",
    "        if match:\n",
    "            # Return the entire matched function code\n",
    "            return match.group(0)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error extracting function code for {function_name}: {e}\", verbose)\n",
    "        return None\n",
    "\n",
    "\n",
    "def can_parse_ast(code):\n",
    "    \"\"\"\n",
    "    Check if the code can be parsed into an AST.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing code into AST: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate_test_case(test_case, global_namespace, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a test case by extracting the boolean expression and evaluating it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove the \"assert \" prefix to get the boolean expression\n",
    "        expression = test_case.replace(\"assert \", \"\")\n",
    "        # Evaluate the expression using eval and the provided global namespace\n",
    "        return eval(expression, global_namespace)\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error evaluating test case {test_case}: {e}\", verbose)\n",
    "        return False\n",
    "\n",
    "\n",
    "def execute_code_and_check_tests(code_file, test_cases, verbose=False):\n",
    "    \"\"\"\n",
    "    Execute the code in the file and check if all test cases pass.\n",
    "    If the code file is invalid or raises an error, return False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(code_file, 'r') as file:\n",
    "            code = file.read()\n",
    "\n",
    "        # Skip if the file is empty\n",
    "        if not code.strip():\n",
    "            log_error(f\"Error: {code_file} is empty.\", verbose)\n",
    "            return False\n",
    "\n",
    "        # Determine the expected function name from the first test case\n",
    "        expected_function_name = test_cases[0].split(\"(\")[0].replace(\"assert \", \"\")\n",
    "\n",
    "        # Check if the entire file can be parsed into an AST\n",
    "        if can_parse_ast(code):\n",
    "            # Use the entire code for testing\n",
    "            global_namespace = {}\n",
    "            try:\n",
    "                exec(code, global_namespace)\n",
    "            except SyntaxError as e:\n",
    "                log_error(f\"Skipping file due to SyntaxError: {e}\", verbose)\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                log_error(f\"Error executing code: {e}\", verbose)\n",
    "                return False\n",
    "        else:\n",
    "            # Extract all function names from the code\n",
    "            function_names = extract_functions_from_code(code, verbose)\n",
    "            if not function_names:\n",
    "                log_error(f\"Error: No functions found in {code_file}.\", verbose)\n",
    "                return False\n",
    "\n",
    "            # Check if the expected function exists in the code\n",
    "            if expected_function_name not in function_names:\n",
    "                log_error(f\"Error: Function {expected_function_name} not found in {code_file}.\", verbose)\n",
    "                return False\n",
    "\n",
    "            # Extract the code for the expected function\n",
    "            function_code = extract_function_code(code, expected_function_name, verbose)\n",
    "            if not function_code:\n",
    "                log_error(f\"Error: Could not extract code for function {expected_function_name}.\", verbose)\n",
    "                return False\n",
    "\n",
    "            # Execute only the extracted function code\n",
    "            global_namespace = {}\n",
    "            try:\n",
    "                exec(function_code, global_namespace)\n",
    "            except SyntaxError as e:\n",
    "                log_error(f\"Skipping function due to SyntaxError: {e}\", verbose)\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                log_error(f\"Error executing function {expected_function_name}: {e}\", verbose)\n",
    "                return False\n",
    "\n",
    "        # Check if all test cases pass with this function\n",
    "        for test_case in test_cases:\n",
    "            if not evaluate_test_case(test_case, global_namespace, verbose):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error executing {code_file}: {e}\", verbose)\n",
    "        return False\n",
    "\n",
    "\n",
    "def calculate_pass_at_1_rate(test_list, code_directory, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the pass@1 rate by checking if the code files pass their respective test cases.\n",
    "    Skip invalid or problematic code files.\n",
    "    \"\"\"\n",
    "    passed = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx, test_cases in enumerate(test_list):\n",
    "        code_file = os.path.join(code_directory, f\"sample{idx}.py\")\n",
    "        if os.path.exists(code_file):\n",
    "            total += 1\n",
    "            if execute_code_and_check_tests(code_file, test_cases, verbose):\n",
    "                passed += 1\n",
    "\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    print(\"pass@1 rate: \" + str(round((passed / total) * 100, 2)) + \"%\")\n",
    "    return (passed / total) * 100\n"
   ],
   "metadata": {
    "id": "BdCsHHT9Q39u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim: int, vocab_size: int,\n",
    "                 filter_sizes: Iterable[int], num_filters: Iterable[int],\n",
    "                 padding_idx: int, gpu: bool = False,\n",
    "                 dropout=0.2, dis_init=\"uniform\") -> None:\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_params(dis_init)\n",
    "\n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get final predictions of discriminator\n",
    "        :param inp: batch_size * seq_len\n",
    "        :return: pred: batch_size * 2\n",
    "        \"\"\"\n",
    "        feature = self.get_feature(inp)\n",
    "        pred = self.feature2out(self.dropout(feature))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def get_feature(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get feature vector of given sentences\n",
    "        :param inp: batch_size * max_seq_len\n",
    "        :return: batch_size * feature_dim\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n",
    "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init_params(self, dis_init: str):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if dis_init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif dis_init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif dis_init == 'truncated_normal':\n",
    "                    truncated_normal_(param, std=stddev)\n",
    "\n",
    "\n",
    "def truncated_normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implemented by @ruotianluo\n",
    "    See https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15\n",
    "    \"\"\"\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "    return tensor\n"
   ],
   "metadata": {
    "id": "kJCHVnThQ39u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import ast\n",
    "import codecs\n",
    "import json\n",
    "import os.path\n",
    "import shutil\n",
    "import gc\n",
    "import tempfile\n",
    "from statistics import mean\n",
    "\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class GANTrainer:\n",
    "\n",
    "    def __init__(self, cfg: Namespace) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.device = \"cuda\" if self.cfg.gpu else \"cpu\"\n",
    "        print(\"device: \", self.device)\n",
    "        print(\" >> init tokenizer\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.base_model, padding_side=\"left\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(\" >> init generator\")\n",
    "        if self.cfg.load_generator:\n",
    "            self.generator = AutoModelForCausalLMWithValueHead.from_pretrained(self.cfg.load_generator_path)\n",
    "        else:\n",
    "            self.generator = AutoModelForCausalLMWithValueHead.from_pretrained(self.cfg.base_model)\n",
    "            self.generator.save_pretrained(self.cfg.gen_dir)\n",
    "        if self.cfg.base_model == \"codeparrot/codeparrot-small\":\n",
    "            self.code_parrot = True\n",
    "        else:\n",
    "            self.code_parrot = False\n",
    "\n",
    "        print(\" >> init discriminator\")\n",
    "        self.discriminator = CNNDiscriminator(embed_dim=self.cfg.embed_dim, vocab_size=len(self.tokenizer),\n",
    "                                              filter_sizes=cfg.filter_sizes, num_filters=cfg.num_filters,\n",
    "                                              padding_idx=self.tokenizer.pad_token_id, gpu=self.cfg.gpu,\n",
    "                                              dropout=self.cfg.dropout)\n",
    "        if self.cfg.load_discriminator:\n",
    "            state_dict = torch.load(self.cfg.load_discriminator_file)\n",
    "            self.discriminator.load_state_dict(state_dict)\n",
    "        self.discriminator = self.discriminator.to(self.device)\n",
    "        self.ppo_cfg = PPOConfig(**{\"batch_size\": self.cfg.batch_size,\n",
    "                                    \"mini_batch_size\": self.cfg.batch_size,\n",
    "                                    \"optimize_device_cache\": False})\n",
    "        self.ppo_trainer = PPOTrainer(config=self.ppo_cfg, model=self.generator, tokenizer=self.tokenizer)\n",
    "        self.generation_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": self.cfg.max_new_tokens,\n",
    "            \"bos_token_id\": 0,\n",
    "            'num_return_sequences': 1\n",
    "        }\n",
    "        self.adv_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.discriminator.parameters(), lr=self.cfg.disc_lr)\n",
    "        print(\" >> read data\")\n",
    "        self.dataset_train = load_gen_data(os.path.join(self.cfg.data_dir, \"mbpp_train.json\"), self.tokenizer,\n",
    "                                           train=True, code_parrot=self.code_parrot, mbpp=True)\n",
    "        self.dataset_test = load_gen_data(os.path.join(self.cfg.data_dir, \"mbpp_test.json\"), self.tokenizer,\n",
    "                                          train=False, code_parrot=self.code_parrot, mbpp=True)\n",
    "\n",
    "    def adversarial_train(self) -> None:\n",
    "        \"\"\"\n",
    "        Trainings process of the generator and the discriminator.\n",
    "        \"\"\"\n",
    "        print(\" >> prepare Generator\")\n",
    "        self.generator = AutoModelForCausalLMWithValueHead.from_pretrained(self.cfg.gen_dir)\n",
    "        self.generator = self.generator.to(self.device)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        print(\" =========== Discriminator Pretraining ===========\")\n",
    "        disc_loss = []\n",
    "        disc_acc = []\n",
    "        disc_loss, disc_acc = self._disc_adv_train(-1, disc_loss, disc_acc)\n",
    "        print(\" > \", {\"loss\": disc_loss[0], \"acc\": disc_acc[0]})\n",
    "        print(\" =========== Start Adversarial Training ===========\")\n",
    "        avg_rewards = []\n",
    "        for epoch in range(self.cfg.num_adv_epochs):\n",
    "            print(f\" --- Epoch {epoch}: Generator ---\")\n",
    "            avg_rewards = self._gen_adv_train(epoch, avg_rewards)\n",
    "            print(\"Average reward: \", avg_rewards[epoch])\n",
    "            print(f\" --- Epoch {epoch}: Discriminator---\")\n",
    "            disc_loss, disc_acc = self._disc_adv_train(epoch, disc_acc, disc_loss)\n",
    "            print(\" > \", {\"loss\": disc_loss[epoch + 1], \"acc\": disc_acc[epoch + 1]})\n",
    "            print(f\" -----------------------------------\")\n",
    "        print(\"Average Rewards for the epochs: \", avg_rewards)\n",
    "\n",
    "        if self.cfg.save_RL:\n",
    "            print(\" >> save RL model\")\n",
    "            model_save_now_path = self.cfg.gen_dir + \"/\" + \"finished_model\"\n",
    "            self.generator.save_pretrained(model_save_now_path + \"/generator\")\n",
    "            torch.save(self.discriminator.state_dict(), os.path.join(model_save_now_path, \"discriminator.pt\"))\n",
    "            data = {\"disc_loss\": disc_loss, \"disc_acc\": disc_acc, \"gen_rewards\": avg_rewards}\n",
    "            #data = {\"gen_rewards\": avg_rewards}\n",
    "            with open(model_save_now_path + \"output.json\", \"w\") as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "\n",
    "        if self.cfg.delete_temp_files:\n",
    "            print(\" >> delete temporary files\")\n",
    "            if os.path.exists('./temp_files'):\n",
    "                shutil.rmtree('./temp_files')\n",
    "\n",
    "    def _gen_adv_train(self, current_epoch: int, avg_rewards: list) -> list:\n",
    "        \"\"\"\n",
    "        Trainings process of the generator.\n",
    "        \"\"\"\n",
    "        data = DataLoader(dataset=self.dataset_train, batch_size=self.cfg.batch_size, shuffle=True, drop_last=True)\n",
    "        padding_length = get_index_length_datasets(train=True, code_parrot=self.code_parrot, mbpp=True)\n",
    "        avg_rewards_during_batches = []\n",
    "        for batch_number, batch in enumerate(data):\n",
    "            prompts_tensor = [prompt.to(torch.int64).to(self.device) for prompt in\n",
    "                              torch.unbind(batch['prompts'], dim=0)]\n",
    "            prompts_txt = self.tokenizer.batch_decode(prompts_tensor, skip_special_tokens=True)\n",
    "\n",
    "            generation_tensor = self.ppo_trainer.generate(query_tensor=prompts_tensor, return_prompt=False,\n",
    "                                                          **self.generation_kwargs)\n",
    "            generated_txts = self.tokenizer.batch_decode(generation_tensor, skip_special_tokens=True)\n",
    "\n",
    "            test_list_tensor = [prompt.to(torch.int64).to(self.device) for prompt in\n",
    "                                torch.unbind(batch['test_list'], dim=0)]\n",
    "            test_list_txt = self.tokenizer.batch_decode(test_list_tensor, skip_special_tokens=True)\n",
    "\n",
    "            temp_file_path = []\n",
    "            for k, sample in enumerate(generated_txts):\n",
    "                try:\n",
    "                    header_prompt = \"\\n# \" + prompts_txt[k] + \"\\n\\n\"\n",
    "                    code = codecs.unicode_escape_decode(sample)[0]\n",
    "                    if not os.path.exists(\"./temp_files\"):\n",
    "                        os.makedirs(\"./temp_files\")\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\", dir=\"./temp_files\",\n",
    "                                                     prefix=\"Epoch \" + str(current_epoch) + \"_\") as temp_file:\n",
    "                        temp_file.write(header_prompt.encode('utf-8'))\n",
    "                        temp_file.write(code.encode('utf-8'))\n",
    "                        temp_file_path.append(temp_file.name)\n",
    "                    break\n",
    "                except UnicodeError:\n",
    "                    pass\n",
    "            rewards, avg_rewards_during_batches = get_rewards(generation_tensor=generation_tensor,\n",
    "                                                              generation_text=generated_txts,\n",
    "                                                              discriminator=self.discriminator,\n",
    "                                                              avg_rewards_during_batches=avg_rewards_during_batches,\n",
    "                                                              tokenizer=self.tokenizer, device=self.device,\n",
    "                                                              padding_length=padding_length,\n",
    "                                                              disc_weight=self.cfg.disc_weight,\n",
    "                                                              temp_file_path=temp_file_path,\n",
    "                                                              test_list=test_list_txt)\n",
    "            rewards = [reward.detach().to(self.device) for reward in rewards]\n",
    "            self.ppo_trainer.step(prompts_tensor, generation_tensor, rewards)\n",
    "            del prompts_tensor\n",
    "            del prompts_txt\n",
    "            del generation_tensor\n",
    "            del generated_txts\n",
    "            del rewards\n",
    "            gc.collect()\n",
    "        self.generator.save_pretrained(os.path.join(self.cfg.gen_dir, \"generator\"))\n",
    "        avg_rewards_during_epoch = mean(avg_rewards_during_batches)  # mean(avg_reward per batch) = avg_reward(epoch)\n",
    "        avg_rewards.append(avg_rewards_during_epoch)\n",
    "        return avg_rewards\n",
    "\n",
    "    def _disc_adv_train(self, current_epoch: int, disc_loss_list: list, disc_acc_list: list) -> tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Trainings process of the discriminator.\n",
    "        \"\"\"\n",
    "        directory_path = \"./save/huggan/gen/generator\"\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "            # Copy the base-model to the generator dir for disc pretraining\n",
    "            source_path_config = \"./save/huggan/gen/config.json\"\n",
    "            source_path_generation_config = \"./save/huggan/gen/generation_config.json\"\n",
    "            source_path_model = \"./save/huggan/gen/model.safetensors\"\n",
    "            shutil.copy(source_path_config, directory_path)\n",
    "            shutil.copy(source_path_generation_config, directory_path)\n",
    "            shutil.copy(source_path_model, directory_path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        lm_generator = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            pretrained_model_name_or_path=self.cfg.gen_dir + \"generator\").to(self.device)\n",
    "\n",
    "        dataset_disc = prepare_data(lm_generator, self.dataset_train,\n",
    "                                    self.cfg.num_samples, self.generation_kwargs,\n",
    "                                    self.tokenizer, train=True, code_parrot=self.code_parrot)\n",
    "        data = DataLoader(dataset=dataset_disc, batch_size=self.cfg.batch_size, shuffle=True, drop_last=True)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        total_num = 0\n",
    "        for batch in data:\n",
    "            samples = Tensor.int(batch[\"code\"]).to(self.device)\n",
    "            classes = batch[\"ground_truth\"]\n",
    "\n",
    "            classes = list(torch.unbind(Tensor.int(classes), dim=0))\n",
    "            classes_decoded_str = self.tokenizer.batch_decode(classes, skip_special_tokens=True)\n",
    "            classes_decoded_int = torch.tensor([int(item) for item in classes_decoded_str], device=self.device,\n",
    "                                               dtype=torch.int64)\n",
    "\n",
    "            classification = self.discriminator.forward(samples)\n",
    "\n",
    "            loss = self.adv_loss(classification, classes_decoded_int)\n",
    "            self._optimize_discriminator(loss)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (classification.argmax(dim=-1) == classes_decoded_int).sum().item()\n",
    "            total_num += len(classes_decoded_int)\n",
    "            del samples\n",
    "            del classes_decoded_int\n",
    "            gc.collect()\n",
    "        total_loss /= len(data)\n",
    "        total_acc /= total_num\n",
    "        torch.save(self.discriminator.state_dict(), os.path.join(self.cfg.disc_dir, \"discriminator.pt\"))\n",
    "        disc_acc_list.append(total_acc)\n",
    "        disc_loss_list.append(total_loss)\n",
    "        return disc_loss_list, disc_acc_list\n",
    "\n",
    "    def _optimize_discriminator(self, loss: _Loss) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), self.cfg.clip_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        print(\">> begin evaluation\")\n",
    "        print(\"> start generating mbpp samples\")\n",
    "        mbpp_testcases = []\n",
    "        for idx in range(self.dataset_test.__len__()):\n",
    "            mbpp_sample_generieren(idx=idx, device=self.device, dataset_test=self.dataset_test,\n",
    "                                   generator=self.ppo_trainer, tokenizer=self.tokenizer,\n",
    "                                   generation_kwargs=self.generation_kwargs, destination_dir_path=\"mbpp_samples\")\n",
    "            loading_bar(idx, self.dataset_test.__len__())\n",
    "            mbpp_testcases.append(self.dataset_test.__getitem__(idx)['test_list'].to(torch.int64))\n",
    "        mbpp_testcases = self.tokenizer.batch_decode(mbpp_testcases, skip_special_tokens=True)\n",
    "        mbpp_testcases = [ast.literal_eval(item) for item in mbpp_testcases]\n",
    "        print(\"\\nAll mbpp test sample generated.\")\n",
    "\n",
    "        humaneval_data = load_gen_data(os.path.join(self.cfg.data_dir, \"humaneval.json\"), self.tokenizer, train=False,\n",
    "                                       code_parrot=self.code_parrot, mbpp=False)\n",
    "        print(\"> start generating humaneval samples\")\n",
    "        human_eval_testcases = []\n",
    "        for idx in range(humaneval_data.__len__()):\n",
    "            mbpp_sample_generieren(idx=idx, device=self.device, dataset_test=humaneval_data,\n",
    "                                   generator=self.ppo_trainer, tokenizer=self.tokenizer,\n",
    "                                   generation_kwargs=self.generation_kwargs, destination_dir_path=\"human_eval_samples\")\n",
    "            loading_bar(idx, humaneval_data.__len__())\n",
    "            human_eval_testcases.append(humaneval_data.__getitem__(idx)['test_list'].to(torch.int64))\n",
    "        human_eval_testcases = self.tokenizer.batch_decode(human_eval_testcases, skip_special_tokens=True)\n",
    "        human_eval_testcases = [ast.literal_eval(item) for item in human_eval_testcases]\n",
    "        print(\"\\nAll human_eval test sample generated.\")\n",
    "\n",
    "        pep8_mbpp, most_common_error_mbpp, compliance_rate_mbpp = eval_pep8(\"./mbpp_samples\")\n",
    "        pep8_human_eval, most_common_error_humaneval, compliance_rate_humaneval = eval_pep8(\"./human_eval_samples\")\n",
    "        pass_at_1_mbpp = calculate_pass_at_1_rate(mbpp_testcases, \"./mbpp_samples\", False)\n",
    "        pass_at_1_human_eval = calculate_pass_at_1_rate(human_eval_testcases, \"./human_eval_samples\", False)\n",
    "        results = {\n",
    "            'test_dataset': [\"mbpp\", \"human_eval\"],\n",
    "            'pep8_average_error': [pep8_mbpp, pep8_human_eval],\n",
    "            'most_common_error': [most_common_error_mbpp, most_common_error_humaneval],\n",
    "            'compliance_rate': [compliance_rate_mbpp, compliance_rate_humaneval],\n",
    "            'pass_at_1': [pass_at_1_mbpp, pass_at_1_human_eval]\n",
    "        }\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv('results.csv', index=False)\n"
   ],
   "metadata": {
    "id": "K37ANhndQ39u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "def load_config(config_path: str = \"config.yaml\") -> Namespace:\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML file and return it as a Namespace object.\n",
    "    \"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "\n",
    "    config = Namespace(**config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    config = load_config()\n",
    "\n",
    "    if not os.path.exists(config.gen_dir):\n",
    "        print(\" > makedirs\", config.gen_dir)\n",
    "        os.makedirs(config.gen_dir, exist_ok=True)\n",
    "    if not os.path.exists(config.disc_dir):\n",
    "        print(\" > makedirs\", config.disc_dir)\n",
    "        os.makedirs(config.disc_dir, exist_ok=True)\n",
    "\n",
    "    trainer = GANTrainer(config)\n",
    "    start_time = time.time()\n",
    "\n",
    "    if config.adversarial:\n",
    "        trainer.adversarial_train()\n",
    "\n",
    "    if config.eval:\n",
    "        trainer.eval()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = (end_time - start_time) / 3600\n",
    "    print(f\"Computing time of training: {elapsed_time:.2f} hours\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXLplMenQ39w",
    "outputId": "ca9eb279-d4e4-4789-fc8c-f1b213def9ba",
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r ./human_eval_samples.zip ./human_eval_samples/\n",
    "!zip -r ./mbpp_samples.zip ./mbpp_samples/\n",
    "#!zip -r ./temp_files.zip ./temp_files/\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"/content/human_eval_samples.zip\")\n",
    "files.download(\"/content/mbpp_samples.zip\")\n",
    "#files.download(\"/content/temp_files.zip\")\n",
    "files.download(\"/content/results.csv\")"
   ],
   "metadata": {
    "id": "mbMasRcZR4r_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r ./generator.zip /content/save/huggan/gen/finished_model\n",
    "files.download(\"/content/generator.zip\")"
   ],
   "metadata": {
    "id": "0Vi2_dTNR5jw"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
